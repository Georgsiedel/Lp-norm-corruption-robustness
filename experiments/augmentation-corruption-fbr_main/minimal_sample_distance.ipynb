{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to calculate the *minimal sample distance* used in *On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness*.  As an example calculation it will produce a plot similar to the left plot in Figure 2b.\n",
    "\n",
    "The notebook requires the CIFAR-10 dataset and GPU-enabled pytorch, as well as the packages listed in `notebook_requirements.txt`.  Over the course of the notebook, it will train one ResNet-40-2s for 100 epochs, which takes around an hour on a single V100 GPU.  The CIFAR-10 dataset should be in the default format used by torchvision.  Please set ```CIFAR_path``` to the proper directory.  If you would like torchvision to automatically download the dataset to this location, set ```download=True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_path = \"path/to/dataset\"\n",
    "download = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to control the GPU device the notebook runs on, set it below.  Otherwise, it will be chosen automatically.  You may also set the number of workers used by the pytorch dataloader for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_device = None\n",
    "dataloader_num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains basic set-up code for the corruptions and dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from notebook_utils.wideresnet import WideResNet\n",
    "from notebook_utils.training_loop import train_model\n",
    "\n",
    "# For sample CIFAR-10-C corruptions\n",
    "from wand.api import library as wandlibrary\n",
    "from wand.image import Image as WandImage\n",
    "from io import BytesIO\n",
    "import skimage as sk\n",
    "import ctypes\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corruption Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Impulse noise* and *motion blur* will be used as example corruption transforms.  Here we reproduce the motion blur and impulse noise corruption functions for CIFAR-10-C used in *Benchmarking Neural Network Robustness to Common Corruptions and Perturbations*.  We separate out the random parameter generation from the transform function so that the same transform may be sampled and applied to multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impulse_noise(x, seed, severity=1):\n",
    "    im_size = 32\n",
    "    c = [.01, .02, .03, .05, .07][int(severity) - 1]\n",
    "\n",
    "    x = sk.util.random_noise(np.array(x) / 255., mode='s&p', amount=c, seed=seed)\n",
    "    return (np.clip(x, 0, 1) * 255).astype(np.float32)\n",
    "\n",
    "def sample_impulse_noise():\n",
    "    return {'seed': np.random.randint(low=0, high=2**32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n",
    "                                              ctypes.c_double,  # radius\n",
    "                                              ctypes.c_double,  # sigma\n",
    "                                              ctypes.c_double)  # angle\n",
    "class MotionImage(WandImage):\n",
    "    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n",
    "        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n",
    "\n",
    "def motion_blur(x, angle, severity=1):\n",
    "    im_size = 32\n",
    "    c = [(6,1), (6,1.5), (6,2), (8,2), (9,2.5)][int(severity) - 1]\n",
    "\n",
    "    output = BytesIO()\n",
    "    x.save(output, format='PNG')\n",
    "    x = MotionImage(blob=output.getvalue())\n",
    "\n",
    "    x.motion_blur(radius=c[0], sigma=c[1], angle=angle)\n",
    "\n",
    "    x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n",
    "                     cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if x.shape != (im_size, im_size):\n",
    "        return np.clip(x[..., [2, 1, 0]], 0, 255).astype(np.float32)  # BGR to RGB\n",
    "    else:  # greyscale to RGB\n",
    "        return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255).astype(np.float32)\n",
    "    \n",
    "def sample_motion_blur():\n",
    "    return {'angle' : np.random.uniform(low=-45, high=45)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_MEAN = [125.3/255, 123.0/255, 113.9/255]\n",
    "CIFAR_STD = [63.0/255, 62.1/255, 66.7/255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = tv.transforms.Compose([\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.RandomCrop(32, padding=4),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "cifar10_default = tv.datasets.CIFAR10(\n",
    "    root=CIFAR_path,\n",
    "    train=True,\n",
    "    transform=train_transform,\n",
    "    download=download\n",
    "    )\n",
    "\n",
    "postprocess = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "cifar10_no_aug = tv.datasets.CIFAR10(\n",
    "    root=CIFAR_path,\n",
    "    train=True,\n",
    "    transform=None,\n",
    "    download=download\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Minimal Sample Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given augmentation and corruption distibutions $p_a$ and $p_c$, the minimal sample distance is defined as $$d_{\\mathrm{MSD}}(p_a, p_c) = \\min_{a \\in \\mathbb{A} \\sim p_a} || f(a)  - \\mathbb{E}_{c \\sim p_c} [ f(c) ] || \\, ,$$ where $f(t)$ encodes an image transform $t$ into a feature space.  First we train this encoder, then we will use it to calculate $d_{\\mathrm{MSD}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Encoder $f(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain an encoding $f(t)$ of a transform $t$, we start with an encoding $\\hat{f}(x)$ of an image $x$.  The transform encoder is then calculated by applying a transform $t$ to a fixed set of datapoints $\\mathbb{D}_S$ in the dataset of interest $$f(t) = \\mathbb{E}_{x \\in \\mathbb{D}_S}  [ \\hat{f} (t(x))) ] \\, .$$\n",
    "\n",
    "The encoder for images, $\\hat{f}(x)$, is obtained by training a ResNet-40-2 on CIFAR-10 and taking the last hidden layer as the feature space.  Note that the model is trained using the default CIFAR-10 augmentation of random crop and flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = WideResNet(depth=40, num_classes=10, widen_factor=2).cuda(device=gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(image_encoder, cifar10_default, num_workers=dataloader_num_workers, gpu_device=gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = torch.stack([postprocess(cifar10_no_aug[0][0]).cuda(device=gpu_device)], axis=0)\n",
    "with torch.no_grad():\n",
    "    _ = image_encoder(example_image)\n",
    "    example_features = image_encoder.features.detach().cpu().numpy()\n",
    "print(example_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode a transform $t$, we sample a fixed set of images $\\mathbb{D}_S$ from the CIFAR-10 training set, without data augmentation.  In experiments we use $|\\mathbb{D}_S|=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indices = np.random.choice(np.arange(len(cifar10_no_aug)), size=100, replace=False)\n",
    "def transform_encoder(transform):\n",
    "    images = [postprocess(transform(cifar10_no_aug[i][0])) for i in image_indices] # Transform each image\n",
    "    images = torch.stack(images, dim=0).cuda(device=gpu_device) # Prepare a batch to evaluate\n",
    "    with torch.no_grad():\n",
    "        _ = image_encoder(images)\n",
    "        image_features = image_encoder.features # Calculate image features\n",
    "        transform_features = torch.mean(image_features, dim=0) # Average over images to get transform features\n",
    "    return transform_features.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of an encoding for a single transform $t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_params = sample_impulse_noise()\n",
    "transform = lambda x: impulse_noise(x, **transform_params, severity=3)\n",
    "transform_features = transform_encoder(transform)\n",
    "print(transform_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Sample Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example of how to compute the minimal sample distance, take the target corruption to be *impulse noise* and the augmentation to be a mixture of *impulse noise* and *motion blur*.  This will produce a plot similar to the left plot of Figure 2b.  First define this augmentation function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x, severity, aug_type, **kwargs):\n",
    "    if aug_type=='motion_blur':\n",
    "        return motion_blur(x, severity=severity, **kwargs)\n",
    "    elif aug_type=='impulse_noise':\n",
    "        return impulse_noise(x, severity=severity, **kwargs)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "def sample_augment(prob_motion_blur):\n",
    "    if np.random.uniform() < prob_motion_blur:\n",
    "        augment_params = {'aug_type': 'motion_blur'}\n",
    "        augment_params.update(sample_motion_blur())\n",
    "    else:\n",
    "        augment_params = {'aug_type': 'impulse_noise'}\n",
    "        augment_params.update(sample_impulse_noise())\n",
    "    return augment_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we take $|\\mathbb{A}|=100\\mathrm{k}$ in the paper, here we will use $|\\mathbb{A}|=1\\mathrm{k}$ for efficiency.  To be concrete, consider an augmentation distribution with a mixing probability of 0.5.  Then calculate the feature space for 1k randomly sampled augmentations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aug_features(motion_blur_probability):\n",
    "    aug_features = []\n",
    "    for i in range(1000):\n",
    "        transform_params = sample_augment(motion_blur_probability)\n",
    "        transform = lambda x: augment(x, severity=3, **transform_params) \n",
    "        feature = transform_encoder(transform)\n",
    "        aug_features.append(feature)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Completed transform {}.\".format(i+1))\n",
    "    return np.stack(aug_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_features = calc_aug_features(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 100 corruptions to approximate averaging over $p_c$ and calculate $\\mathbb{E}_{c \\sim p_c} [ f(c) ]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_avg():\n",
    "    corr_features = []\n",
    "    for i in range(100):\n",
    "        transform_params = sample_impulse_noise()\n",
    "        transform = lambda x: impulse_noise(x, severity=3, **transform_params) \n",
    "        feature = transform_encoder(transform)\n",
    "        corr_features.append(feature)\n",
    "    return np.mean(np.stack(corr_features, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_avg = calc_corr_avg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain $d_{\\mathrm{MSD}}(p_a, p_c) = \\min_{a \\in \\mathbb{A} \\sim p_a} || f(a)  - \\mathbb{E}_{c \\sim p_c} [ f(c) ] || $, we only need to calculate the L-2 distance to each augmentation feature and take the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.linalg.norm(aug_features-corr_avg.reshape(1,-1), axis=1)\n",
    "msd = np.min(dists, axis=0)\n",
    "print(\"MSD between impulse noise corruption and 50/50 impulse-noise/motion-blur augmentation: {}\".format(msd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a plot like the left part of Figure 2b, we need to repeat this for multiple mixing fractions.  For comparison, we may also calculate the distance between distribution centers, which here we call MMD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dists_for_mixture(motion_blur_prob):\n",
    "    print(\"Starting calculation for mixing fraction {}.\".format(motion_blur_prob))\n",
    "    aug_features = calc_aug_features(motion_blur_prob)\n",
    "    corr_avg = calc_corr_avg()\n",
    "    msd_dists = np.linalg.norm(aug_features-corr_avg.reshape(1,-1), axis=1)\n",
    "    msd = np.min(msd_dists, axis=0)\n",
    "    aug_avg = np.mean(aug_features, axis=0)\n",
    "    mmd = np.linalg.norm(aug_avg - corr_avg)\n",
    "    return (msd, mmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_dists = {i : dists_for_mixture(i) for i in [0.0, 0.5, 0.9, 0.99, 1.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting MSD and MMD yields the result from Fire 2b.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(list(msd_dists.keys()), [i[1] for i in msd_dists.values()], label=\"MMD\")\n",
    "plt.plot(list(msd_dists.keys()), [i[0] for i in msd_dists.values()], label=\"MSD\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Motion Blur Fraction in Augmentation\")\n",
    "plt.ylabel(\"Distance to Impulse Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\n",
    "The `impulse noise` and `motion_blur` functions are adapted from code licensed under the license at `third_party/imagenetc_license`, accessible from the root directory of this source tree. The original source is available at `https://github.com/hendrycks/robustness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
